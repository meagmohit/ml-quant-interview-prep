# Resources for Interview Preparation
Preparation material and resources for the ML (including DL) and Quant Research interviews
--- 

## Topics [ML-Standard]
--- 
* Optimization in NNs: [[CS231n-opt1]](https://cs231n.github.io/optimization-1/) [[CS231n-opt2]](https://cs231n.github.io/optimization-2/) [[Opt-Algos]](https://ruder.io/optimizing-gradient-descent/)
* Basics and NNs: [[CS231n-classify-1]](https://cs231n.github.io/classification/) [[CS231n-classify-2]](https://cs231n.github.io/linear-classify/) [[CS231n-nn1]](https://cs231n.github.io/neural-networks-1/) [[CS231n-nn2]](https://cs231n.github.io/neural-networks-2/) [[CS231n-nn3]](https://cs231n.github.io/neural-networks-3/) [[L1/L2-explained]](https://explained.ai/regularization/index.html) [[Dropout-explained]](https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275)
* Batch Normalization: [[Backward pass using graph]](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html) [[Backward pass with derivatives]](http://cthorey.github.io./backpropagation/) [[Video: BN-Training]](https://www.youtube.com/watch?v=nUUqwaxLnWs) [[Video: BN-Testing]](https://www.youtube.com/watch?v=5qefnAek8OA)
* Decision Trees/Random Forests: [ISLR Chap-8]
* Principal Component Analysis(PCA): [[PCA Tutorial]](https://arxiv.org/pdf/1404.1100.pdf) [ISLR 10.2] [[PCA-2]](http://cs229.stanford.edu/notes/cs229-notes10.pdf)
* Clustering: [ISLR Chap-10] [[k-means]](http://cs229.stanford.edu/notes/cs229-notes7a.pdf) [[Clustering-CMU]](http://www.cs.cmu.edu/~ninamf/courses/601sp15/slides/21_clustering_4-6-2015.pdf) 
* ICA/FA: [[ICA]](http://cs229.stanford.edu/notes/cs229-notes11.pdf) [[FA]](http://cs229.stanford.edu/notes/cs229-notes9.pdf)
* Classification: [[ISLR-Chap4]] [[LR/NB]](http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)
* Regression: [[ISLR-Chap3]]
* Boosting: [[Adaboost]](https://www.cs.princeton.edu/~schapire/talks/nips-tutorial.pdf) [[Gradient Boosting-1]](https://www.gormanalysis.com/blog/gradient-boosting-explained/) [[Gradient Boosting-2]](https://explained.ai/gradient-boosting/index.html)

## Topics [Maths/Probs/Stats]
---
* Multi-variate Gaussians:  [[Gaussians-1]](http://cs229.stanford.edu/section/cs229-moregaussians.pdf) [[Gaussians-2]](http://cs229.stanford.edu/section/cs229-gaussians.pdf)
* Sampling: [[CDF-INV]](https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/simulation/) [[Box-Mueller/Marsaglia-Polar]](https://www.alanzucconi.com/2015/09/16/how-to-sample-from-a-gaussian-distribution/) [[Detailed Maths]](https://statweb.stanford.edu/~owen/mc/Ch-nonunifrng.pdf) [[Gaussian-Inv,BM]](https://www.alanzucconi.com/2015/09/16/how-to-sample-from-a-gaussian-distribution/) [[Sampling uniformly on sphere]](http://corysimon.github.io/articles/uniformdistn-on-sphere/)
* Sample Mean/variances of Gaussian: [[Distribution of sample variance]](https://online.stat.psu.edu/stat414/lesson/26/26.3) 

## Topics [DL]
---
* CNNs: [[Basics]](https://cs231n.github.io/convolutional-networks/) [[Visualizing CNNs]](https://cs231n.github.io/understanding-cnn/) [[Fine-tuning CNNs]](https://cs231n.github.io/transfer-learning/) [[CNN Backprop/Architectures Chap8]](http://snasiriany.me/files/ml-book.pdf)
* NLP: [[Word Embeddings/Word2Vec]](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)
## Topics [Development/Python]
---
* Python call by value or reference? [[1]](http://foobarnbaz.com/2012/07/08/understanding-python-variables/) [[2]](https://jeffknupp.com/blog/2012/11/13/is-python-callbyvalue-or-callbyreference-neither/) [[3]](http://stupidpythonideas.blogspot.com/2013/11/does-python-pass-by-value-or-by.html) [[4]](https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/)
* How does Python work? Interpreter vs Compiler? [[1]](https://www.codingdojo.com/blog/interpreters-run-python-code) [[2]](https://towardsdatascience.com/how-does-python-work-6f21fd197888)
* Memory Management in Python: [[1]](https://www.geeksforgeeks.org/memory-management-in-python/)
* Dictionaries Implementation in Python: [[1]](https://stackoverflow.com/questions/327311/how-are-pythons-built-in-dictionaries-implemented) 
* OOP in Python: [[1]](https://www.programiz.com/python-programming/object-oriented-programming)

## System Design
---
* ML System Design: [[1]](https://github.com/chiphuyen/machine-learning-systems-design/blob/master/build/build1/consolidated.pdf)

## Old [Archive]
---

## Data Structures and Algorithms
* Basic DS and Algo in Python [[Online Book]](https://runestone.academy/runestone/books/published/pythonds/index.html)
* Leetcode problems

---
## Basic Machine Learning
**Resources:**
* CMU Spring'16 by Tom Mitchell and Nina [[Course Link]](http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml)
* An Introduction to Statistical Learning [[Book]](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)
* Pre-requisites: [[Matrix Derivatives]](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf) [[Derivatives Cheatsheet]](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf)
* CS229 Review Notes (Pre-reqs): [[Probability]](http://cs229.stanford.edu/section/cs229-prob.pdf) [[Linear Algebra]](http://cs229.stanford.edu/section/cs229-linalg.pdf)  [[Convex Optimization-1]](http://cs229.stanford.edu/section/cs229-cvxopt.pdf) [[2]](http://cs229.stanford.edu/section/cs229-cvxopt2.pdf)  [[Gaussians-1]](http://cs229.stanford.edu/section/cs229-moregaussians.pdf) [[Gaussians-2]](http://cs229.stanford.edu/section/cs229-gaussians.pdf) [[Gaussian Processes]](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)
* CS229 Review Notes: [[Backprop]](http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf) [[DL]](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes-deep_learning.pdf) [[Decision Trees]](http://cs229.stanford.edu/notes/cs229-notes-dt.pdf) [[Ensemble Methods]](http://cs229.stanford.edu/notes/cs229-notes-ensemble.pdf) [[Linear Regression and Classification (Supervised)]](http://cs229.stanford.edu/notes/cs229-notes1.pdf) [[Generative Learning]](http://cs229.stanford.edu/notes/cs229-notes2.pdf) [[SVM]](http://cs229.stanford.edu/notes/cs229-notes3.pdf) [[Learning Theory]](http://cs229.stanford.edu/notes/cs229-notes4.pdf) [[Regularization and Model Selection]](http://cs229.stanford.edu/notes/cs229-notes5.pdf) [[Perceptron]](http://cs229.stanford.edu/notes/cs229-notes6.pdf) [[k-means]](http://cs229.stanford.edu/notes/cs229-notes7a.pdf) [[GMMs]](http://cs229.stanford.edu/notes/cs229-notes7b.pdf) [[EM]](http://cs229.stanford.edu/notes/cs229-notes8.pdf) [ [[RL]](http://cs229.stanford.edu/notes/cs229-notes12.pdf) [[LQR, DDP and LQG]](http://cs229.stanford.edu/notes/cs229-notes13.pdf) [[Boosting]](http://cs229.stanford.edu/notes/cs229-notes-all/boosting.pdf) [[HMM Notes]](http://cs229.stanford.edu/section/cs229-hmm.pdf) [[Evaluation Metrics]](http://cs229.stanford.edu/section/evaluation_metrics_spring2020.pdf) 
* Other topics: [[Anomaly Detection]](http://courses.washington.edu/css581/lecture_slides/18_anomaly_detection.pdf) (find better link?)
* Topics:
  * Dimensionality Reduction: [[PCA]](https://arxiv.org/pdf/1404.1100.pdf) [[PCA]](http://cs229.stanford.edu/notes/cs229-notes10.pdf) [[ICA]](http://cs229.stanford.edu/notes/cs229-notes11.pdf) [[Factor Analysis]](http://cs229.stanford.edu/notes/cs229-notes9.pdf)
  * Regularization: [Summary](https://cs231n.github.io/neural-networks-2/#reg) [[L1/L2]](https://explained.ai/regularization/index.html) [[Dropout]](https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275) 

---
## Deep Learning
**Resources:**
* CS231 Assignment Soln Ref: [[ref-tf-1]](https://github.com/MahanFathi/CS231) [[ref-pytorch-1]](https://github.com/srinadhu/CS231n/) [[ref-3]](https://github.com/jariasf/CS231n) [[ref-4]](https://github.com/Arnav0400/CS231n-2019)
* Basics/Losses: [[Image Classification]](https://cs231n.github.io/classification/) [[Linear Classification]](https://cs231n.github.io/linear-classify/)
* Backprop: [[Optimization-1]](https://cs231n.github.io/optimization-1/) [[Optimization-2]](https://cs231n.github.io/optimization-2/) [[Optimizing GD]](https://ruder.io/optimizing-gradient-descent/) [[Appendix1:Linear Example]](http://cs231n.stanford.edu/handouts/linear-backprop.pdf) [[Appendix2:Derivatives]](http://cs231n.stanford.edu/handouts/derivatives.pdf) [[Appendix3:Vector Derivatives]](http://cs231n.stanford.edu/vecDerivs.pdf)
* NNs: [[NN-1]](https://cs231n.github.io/neural-networks-1/) [[NN-2]](https://cs231n.github.io/neural-networks-2/) [[NN-3]](https://cs231n.github.io/neural-networks-2/) [[NN-case study]](https://cs231n.github.io/neural-networks-case-study/)
* RNNs and LSTMs: [[LSTMs]](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) 
* NLP: [[Word Vectors-I]](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf) [[Word Vectors-II]](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf) [[NNs]](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes03-neuralnets.pdf) [[RNN/LSTM-1]](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf) [[Neural Machine Translation, Seq2Seq, Attention]](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf)
* RL: [[Basics: David Silver Course](https://www.davidsilver.uk/teaching/) Deep RL - DQN and variants: [[1]](https://greentec.github.io/reinforcement-learning-first-en/) [[2]](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df) [[3-PER]](https://danieltakeshi.github.io/2019/07/14/per/) [[RL-Hard]](https://www.alexirpan.com/2018/02/14/rl-hard.html)

ToDO: Add (i)Other networks like BiGRU etc-- find a summary article, (ii) VAEs/GANs, (iii) Word2VEc, and word representations, (iv) implementation of basic models of MNIST/CIFAR-10 models using python for NNs and CNNs, char-rnn models for LSTM, RL playing game model, (v) batch norm

unsorted links: 
1. charrnn: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
2. https://deepgenerativemodels.github.io/notes/vae/
3. MS_Sharma links
4. Really great Stats/ML: http://www.stat.cmu.edu/~cshalizi/uADA/12/

---
## Mathematics
**Resources:**
* Quantitative Finance Interview Question Bank: [[Green Book]](https://www.amazon.com/Practical-Guide-Quantitative-Finance-Interviews/dp/1438236662) Chapter 4, 5.1-5.3, 3.6, 7, 2
* Probability and Statistics: Introduction to Probability by Bertsekas [[Book]](http://ece307.cankaya.edu.tr/uploads/files/introduction%20to%20probability%20(bertsekas,%202nd,%202008).pdf) [[Lecture Notes]](https://www.vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf) [[Summary Notes]](https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/MITRES_6_012S18_Textbook.pdf) [[Solutions]](https://inst.eecs.berkeley.edu/~ee126/sp12/solutions%20to%20bertsekas-book.pdf)
* Linear Algebra (not final): [[Review]](http://cs229.stanford.edu/section/cs229-linalg.pdf) [[Slides]](https://web.mst.edu/~jcmcfd/3108-Slides-v2.pdf) [[Notes]](http://home.iitk.ac.in/~arlal/MTH102/la.pdf) [[Comprehensive Book]](https://math.mit.edu/~gs/linearalgebra/)
* Convex Optimization (Not doing): [[Notes-1]](http://cs229.stanford.edu/section/cs229-cvxopt.pdf) [[Notes-2]](http://cs229.stanford.edu/section/cs229-cvxopt2.pdf) [[Comprehensive Book]](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)

---
## Mathematics
**Resources:**
* Linear Algebra etc.

# Comprehensive Topic List
**DS/Algorithms:** Recursion, DP, Strings, ...
**Statistics in Python:** [[Short-pyStats]](https://realpython.com/python-statistics/) [[Long-pystats]](http://people.duke.edu/~ccc14/sta-663-2017/01_Introduction_To_Python.html) [[Coursera]](https://www.coursera.org/learn/inferential-statistical-analysis-python/supplement/iDA5p/about-our-datasets)


